{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert finetuning\n",
    "Pytorch + TensorFlow 2.0\n",
    "\n",
    "https://github.com/huggingface/transformers#quick-tour-tf-20-training-and-pytorch-interoperability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset glue (/Users/shivangi/tensorflow_datasets/glue/mrpc/0.0.2)\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from /Users/shivangi/tensorflow_datasets/glue/mrpc/0.0.2\n",
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset, tokenizer, model from pretrained model/vocabulary\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-cased')\n",
    "data = tensorflow_datasets.load('glue/mrpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for GLUE as a tf.data.Dataset instance\n",
    "train_dataset = glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, task='mrpc')\n",
    "valid_dataset = glue_convert_examples_to_features(data['validation'], tokenizer, max_length=128, task='mrpc')\n",
    "train_dataset = train_dataset.shuffle(100).batch(32).repeat(2)\n",
    "valid_dataset = valid_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 165s 17s/step - loss: 0.6168 - accuracy: 0.6687 - val_loss: 0.6013 - val_accuracy: 0.6838\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate using tf.keras.Model.fit()\n",
    "\n",
    "# history = model.fit(train_dataset, epochs=3, steps_per_epoch=115,\n",
    "#                     validation_data=valid_dataset, validation_steps=7)\n",
    "history = model.fit(train_dataset, epochs=1, steps_per_epoch=10,\n",
    "                    validation_data=valid_dataset, validation_steps=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorFlow model in PyTorch for inspection\n",
    "model.save_pretrained('./save/')\n",
    "pytorch_model = BertForSequenceClassification.from_pretrained('./save/', from_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_1 is a paraphrase of sentence_0\n",
      "sentence_2 is a paraphrase of sentence_0\n"
     ]
    }
   ],
   "source": [
    "# Quickly test a few predictions - MRPC is a paraphrasing task, let's see if our model learned the task\n",
    "sentence_0 = \"This research was consistent with his findings.\"\n",
    "sentence_1 = \"His findings were compatible with this research.\"\n",
    "sentence_2 = \"His findings were not compatible with this research.\"\n",
    "inputs_1 = tokenizer.encode_plus(sentence_0, sentence_1, add_special_tokens=True, return_tensors='pt')\n",
    "inputs_2 = tokenizer.encode_plus(sentence_0, sentence_2, add_special_tokens=True, return_tensors='pt')\n",
    "\n",
    "pred_1 = pytorch_model(inputs_1['input_ids'], token_type_ids=inputs_1['token_type_ids'])[0].argmax().item()\n",
    "pred_2 = pytorch_model(inputs_2['input_ids'], token_type_ids=inputs_2['token_type_ids'])[0].argmax().item()\n",
    "\n",
    "print(\"sentence_1 is\", \"a paraphrase\" if pred_1 else \"not a paraphrase\", \"of sentence_0\")\n",
    "print(\"sentence_2 is\", \"a paraphrase\" if pred_2 else \"not a paraphrase\", \"of sentence_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
