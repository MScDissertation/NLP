{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul 10 00:51:38 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:17:00.0 Off |                  N/A |\r\n",
      "|  0%   32C    P8    10W / 275W |      2MiB / 11178MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:65:00.0 Off |                  N/A |\r\n",
      "|  0%   31C    P8    10W / 275W |     18MiB / 11176MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    1      2159      G   /usr/lib/xorg/Xorg                             9MiB |\r\n",
      "|    1      2304      G   /usr/bin/gnome-shell                           6MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# Check that we have a GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that PyTorch sees it\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../models’: File exists\n",
      "mkdir: cannot create directory ‘../models/smallBERTa’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir ../models\n",
    "!mkdir ../models/smallBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config arguments\n",
    "<pre>Args:\n",
    "vocab_size (:obj:`int`, optional, defaults to 30522):\n",
    "    Vocabulary size of the BERT model. Defines the different tokens that\n",
    "    can be represented by the `inputs_ids` passed to the forward method of :class:`~transformers.BertModel`.\n",
    "hidden_size (:obj:`int`, optional, defaults to 768):\n",
    "    Dimensionality of the encoder layers and the pooler layer.\n",
    "num_hidden_layers (:obj:`int`, optional, defaults to 12):\n",
    "    Number of hidden layers in the Transformer encoder.\n",
    "num_attention_heads (:obj:`int`, optional, defaults to 12):\n",
    "    Number of attention heads for each attention layer in the Transformer encoder.\n",
    "intermediate_size (:obj:`int`, optional, defaults to 3072):\n",
    "    Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n",
    "hidden_act (:obj:`str` or :obj:`function`, optional, defaults to \"gelu\"):\n",
    "    The non-linear activation function (function or string) in the encoder and pooler.\n",
    "    If string, \"gelu\", \"relu\", \"swish\" and \"gelu_new\" are supported.\n",
    "hidden_dropout_prob (:obj:`float`, optional, defaults to 0.1):\n",
    "    The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.\n",
    "attention_probs_dropout_prob (:obj:`float`, optional, defaults to 0.1):\n",
    "    The dropout ratio for the attention probabilities.\n",
    "max_position_embeddings (:obj:`int`, optional, defaults to 512):\n",
    "    The maximum sequence length that this model might ever be used with.\n",
    "    Typically set this to something large just in case (e.g., 512 or 1024 or 2048).\n",
    "type_vocab_size (:obj:`int`, optional, defaults to 2):\n",
    "    The vocabulary size of the `token_type_ids` passed into :class:`~transformers.BertModel`.\n",
    "initializer_range (:obj:`float`, optional, defaults to 0.02):\n",
    "    The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
    "layer_norm_eps (:obj:`float`, optional, defaults to 1e-12):\n",
    "    The epsilon used by the layer normalization layers.\n",
    "gradient_checkpointing (:obj:`bool`, optional, defaults to False):\n",
    "    If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "config = {\n",
    "    \"model_type\":\"bert\",\n",
    "    \"attention_probs_dropout_prob\": 0.1,\n",
    "    \"hidden_act\": \"gelu\",\n",
    "    \"hidden_dropout_prob\": 0.3,\n",
    "    \"hidden_size\": 768,\n",
    "    \"num_attention_heads\": 12,\n",
    "    \"num_hidden_layers\": 6,\n",
    "    \"vocab_size\": 5000,\n",
    "    \"intermediate_size\": 256,\n",
    "    \"max_position_embeddings\": 256\n",
    "}\n",
    "\n",
    "with open(\"../models/smallBERTa/config.json\", 'w') as fp:\n",
    "    json.dump(config, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path='../wikitext-2-raw/wiki.train.raw'\n",
    "TEST_FILE='../wikitext-2-raw/wiki.test.raw'\n",
    "eval_path= '../wikitext-2-raw/wiki.valid.raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../models/smallBERTa/weights’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "#Setting environment variables\n",
    "os.environ[\"train_path\"] = train_path\n",
    "os.environ[\"eval_path\"] = eval_path\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]='1'  #Makes for easier debugging (just in case)\n",
    "weights_dir = \"../models/smallBERTa/weights\"\n",
    "!mkdir {weights_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tokenizers import ByteLevelBPETokenizer\n",
    "# tokenizer = ByteLevelBPETokenizer()\n",
    "# tokenizer.train([train_path], vocab_size=20000)\n",
    "# !export TOKENIZERS_PARALLELISM=false\n",
    "# from tokenizers import ByteLevelBPETokenizer\n",
    "# from tokenizers.processors import BertProcessing\n",
    "# # Initialize a tokenizer\n",
    "# tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# vocab_size=5000\n",
    "# # Customize training\n",
    "# tokenizer.train(files=train_path, vocab_size=vocab_size, min_frequency=5, special_tokens=[\n",
    "#     \"<s>\",\n",
    "#     \"<pad>\",\n",
    "#     \"</s>\",\n",
    "#     \"<unk>\",\n",
    "#     \"<mask>\",\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5559557ad1d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../models/smallBERTa\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"smallBERTa\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ai_power/lib/python3.6/site-packages/tokenizers/implementations/base_tokenizer.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, pretty)\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0mA\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdestination\u001b[0m \u001b[0mTokenizer\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \"\"\"\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tokenizer.save(\"../models/smallBERTa\", \"smallBERTa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat '../models/smallBERTa/smallBERTa-vocab.json': No such file or directory\n",
      "mv: cannot stat '../models/smallBERTa/smallBERTa-merges.txt': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# !mv ../models/smallBERTa/smallBERTa-vocab.json ../models/smallBERTa/vocab.json\n",
    "# !mv ../models/smallBERTa/smallBERTa-merges.txt ../models/smallBERTa/merges.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = '''python ../transformers/examples/language-modeling/run_language_modeling.py --output_dir {0}  \\\n",
    "    --model_type bert \\\n",
    "    --mlm \\\n",
    "    --train_data_file {1} \\\n",
    "    --eval_data_file {2} \\\n",
    "    --config_name ../models/smallBERTa \\\n",
    "    --tokenizer_name bert-base-uncased \\\n",
    "    --do_train \\\n",
    "    --line_by_line \\\n",
    "    --overwrite_output_dir \\\n",
    "    --do_eval \\\n",
    "    --block_size 256 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --save_steps 10000 \\\n",
    "    --logging_steps 1000 \\\n",
    "    --per_gpu_eval_batch_size 32 \\\n",
    "    --per_gpu_train_batch_size 32 \\\n",
    "    --evaluate_during_training \\\n",
    "    --seed 42 \\\n",
    "    '''.format(weights_dir, train_path, eval_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/10/2020 00:51:50 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
      "07/10/2020 00:51:50 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n",
      "07/10/2020 00:51:50 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='../models/smallBERTa/weights', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=32, per_gpu_eval_batch_size=32, gradient_accumulation_steps=1, learning_rate=0.0001, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jul10_00-51-50_fritz', logging_first_step=False, logging_steps=1000, save_steps=10000, save_total_limit=2, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)\n",
      "07/10/2020 00:51:50 - INFO - transformers.configuration_utils -   loading configuration file ../models/smallBERTa/config.json\n",
      "07/10/2020 00:51:50 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.3,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 256,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 256,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 5000\n",
      "}\n",
      "\n",
      "07/10/2020 00:51:50 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/shivangi/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "07/10/2020 00:51:50 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "07/10/2020 00:51:51 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/shivangi/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "07/10/2020 00:51:51 - INFO - __main__ -   Training new model from scratch\n",
      "/home/shivangi/ai_power/lib/python3.6/site-packages/transformers/modeling_auto.py:709: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "07/10/2020 00:51:51 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at ../wikitext-2-raw/wiki.train.raw\n",
      "07/10/2020 00:52:12 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at ../wikitext-2-raw/wiki.valid.raw\n",
      "07/10/2020 00:52:15 - INFO - transformers.trainer -   You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
      "07/10/2020 00:52:15 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "07/10/2020 00:52:15 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "07/10/2020 00:52:15 - INFO - transformers.trainer -   ***** Running training *****\n",
      "07/10/2020 00:52:15 - INFO - transformers.trainer -     Num examples = 23767\n",
      "07/10/2020 00:52:15 - INFO - transformers.trainer -     Num Epochs = 1\n",
      "07/10/2020 00:52:15 - INFO - transformers.trainer -     Instantaneous batch size per device = 8\n",
      "07/10/2020 00:52:15 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "07/10/2020 00:52:15 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
      "07/10/2020 00:52:15 - INFO - transformers.trainer -     Total optimization steps = 372\n",
      "Epoch:   0%|                                              | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                        | 0/372 [00:00<?, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "#from transformers import DataCollatorForPermutationLanguageModeling\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
